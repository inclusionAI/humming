#pragma once

#include <humming/arith/exp_offset.cuh>
#include <humming/datatype/base_conversion.cuh>
#include <humming/datatype/dequant.cuh>
#include <humming/datatype/dtypes.cuh>
#include <humming/utils/all.cuh>


template <
    class MmaOpClass,
    class BlockShape, class WarpShape,
    class ElementA, class ElementB, class ElementC, class ElementBS,
    class QuantParamConfig>
class MainloopArithmetic : F16Conversion<ElementC> {
private:
  using scalar_t = typename F16Conversion<ElementC>::scalar_t;
  using scalar_t2 = typename F16Conversion<ElementC>::scalar_t2;
  using ValTypeC = typename MmaOpClass::ValTypeC;
  using MmaShape = class MmaOpClass::MmaShape;

  static constexpr bool kUseWgmma = MmaOpClass::kMmaType == MmaType::WGMMA;
  static constexpr bool kIsF16Accum = MmaOpClass::kCTypeBits == 16;
  static constexpr uint32_t kPartMmaShapeK = 256 / ElementA::kBits;

  static constexpr bool kHasInputScale = QuantParamConfig::kHasInputScale;
  static constexpr bool kHasWeightScale = QuantParamConfig::kHasWeightScale;
  static constexpr bool kIsGroupInputScale = kHasInputScale && QuantParamConfig::kInputScaleGroupSize > 0;
  static constexpr bool kIsGroupWeightScale = kHasWeightScale && QuantParamConfig::kWeightScaleGroupSize > 0;
  static constexpr bool kIsChannelInputScale = kHasInputScale && QuantParamConfig::kInputScaleGroupSize == 0;
  static constexpr bool kIsChannelWeightScale = kHasWeightScale && QuantParamConfig::kWeightScaleGroupSize == 0;
  static constexpr bool kHasDynamicZeroPoint = QuantParamConfig::kHasDynamicZeroPoint;

  static constexpr uint32_t kInputScaleGroupSize = kIsGroupInputScale ? QuantParamConfig::kInputScaleGroupSize : 1;
  static constexpr uint32_t kWeightScaleGroupSize = kIsGroupWeightScale ? QuantParamConfig::kWeightScaleGroupSize : 1;
  static constexpr uint32_t kNumGroupsA = kIsGroupInputScale ? CEIL_DIV(kPartMmaShapeK, kInputScaleGroupSize) : 1;
  static constexpr uint32_t kNumGroupsB = kIsGroupWeightScale ? CEIL_DIV(kPartMmaShapeK, kWeightScaleGroupSize) : 1;
  static constexpr uint2 kExpOffset = get_mainloop_exp_offset<
      ElementA, ElementB, ElementBS, kHasDynamicZeroPoint,
      kIsF16Accum, kIsGroupInputScale, kIsGroupWeightScale>();

  static constexpr uint32_t kDequantBSBits = (ElementA::kBits < 16 && !kIsF16Accum) ? 32 : 16;
  static constexpr uint32_t kNumSubBlocksM = WarpShape::M / 16;
  static constexpr uint32_t kNumSubBlocksN = WarpShape::N / 16;
  static constexpr uint32_t kNumASPerSubBlock = kUseWgmma ? 4 : 2;
  static constexpr uint32_t kNumBSPerSubBlock = !kUseWgmma && ElementA::kBits < 16 ? 4 : 2;
  static constexpr uint32_t kNumASPerGroup = kNumSubBlocksM * kNumASPerSubBlock;
  static constexpr uint32_t kNumBSPerGroup = kNumSubBlocksN * kNumBSPerSubBlock;

public:
  uint32_t as[2][kNumASPerGroup][kNumGroupsA];
  uint32_t q_as[kNumASPerGroup][kNumGroupsA];
  uint32_t bs[2][kNumGroupsB][MAX(kNumBSPerGroup, 8) * ElementBS::kBits / 32];
  uint32_t dq_bs[kNumGroupsB][MAX(kNumBSPerGroup, 8) * kDequantBSBits / 32];

  uint32_t zp[2][kNumGroupsB][CEIL_DIV(ElementB::kBits, 4)];

  uint32_t _dummy;

  CUDA_INLINE
  uint4 prepare_zp_for_dequant(uint32_t buffer_id, uint32_t index) {
    uint32_t zp_vals[4] = {0, 0, 0, 0};
    constexpr uint32_t kNumZPBits = 4 * CEIL_DIV(ElementB::kBits, 4);
    constexpr uint32_t kNumZPsPerInt = 32 / kNumZPBits;

    buffer_id = kIsChannelWeightScale ? 0 : buffer_id;

    if constexpr (kHasDynamicZeroPoint) {
      PRAGMA_UNROLL
      for (uint32_t i = 0; i < 2; i++) {
        PRAGMA_UNROLL
        for (uint32_t j = 0; j < 2; j++) {
          uint32_t n_id = 0;
          if (WarpShape::N == 64 && ElementB::kBits > 4 && index >= 2) n_id = 1;

          uint32_t group_id = i % kNumGroupsB;
          uint32_t dzp_val = zp[buffer_id][group_id][n_id];
          dzp_val = dzp_val >> ((index * 2 + j) % kNumZPsPerInt * kNumZPBits);
          zp_vals[j * 2 + i] = dequant_single_zero_point<ElementB, ElementA>(dzp_val);
        }
      }
    }

    return *reinterpret_cast<uint4 *>(zp_vals);
  };

  CUDA_INLINE
  void may_process_bs_and_zp_before_apply_on_b(uint32_t j, uint32_t buffer_id) {
    if (j % 2 == 0) {
      if constexpr (ElementA::kBits == 16 && ElementBS::kBits == 8 && kIsGroupWeightScale) {
        PRAGMA_UNROLL
        for (uint32_t i = 0; i < kNumGroupsB; i++) {
          dequant<ElementBS, ElementA>(bs[buffer_id][i], dq_bs[i], 0);

          scalar_t *dq_bs_scalar_ptr = reinterpret_cast<scalar_t *>(dq_bs + i);

          PRAGMA_UNROLL
          for (uint32_t j = 0; j < 2; j++) {
            scalar_t tmp = dq_bs_scalar_ptr[2 + 4 * j];
            dq_bs_scalar_ptr[2 + 4 * j] = dq_bs_scalar_ptr[1 + 4 * j];
            dq_bs_scalar_ptr[1 + 4 * j] = tmp;
          }

          const scalar_t2 scale_factor = prepare_exp_scale_factor<scalar_t2, kExpOffset.y>();
          scalar_t2 *dq_bs_scalar2_ptr = reinterpret_cast<scalar_t2 *>(dq_bs + i);

          PRAGMA_UNROLL
          for (uint32_t j = 0; j < 4; j++) {
            dq_bs_scalar2_ptr[j] = __hmul2(dq_bs_scalar2_ptr[j], scale_factor);
          }
        };
      };
    };
  };

  CUDA_INLINE
  void may_apply_bs_and_zp_on_b(uint32_t *regs_b, uint32_t j, uint32_t buffer_id) {

    may_process_bs_and_zp_before_apply_on_b(j, buffer_id);

    if constexpr (ElementA::kBits == 16 && kExpOffset.x) {
      uint32_t exp_val = kExpOffset.x + ((1 << (ElementA::kExponentBits - 1)) - 1);
      uint32_t scale_factor_uint = 0x00010001 * (exp_val << ElementA::kMantissaBits);
      scalar_t2 scale_factor = *reinterpret_cast<scalar_t2 *>(&scale_factor_uint);
      scalar_t2 *b_f16_ptr = reinterpret_cast<scalar_t2 *>(regs_b);

      PRAGMA_UNROLL
      for (uint32_t i = 0; i < 4; i++) {
        b_f16_ptr[i] = __hmul2(b_f16_ptr[i], scale_factor);
      }
    }

    // apply bs and/or zp only when we use fp16/bf16 activation.
    if constexpr (ElementA::kBits == 16 && kIsGroupWeightScale) {

      scalar_t2 *b_f16_ptr = reinterpret_cast<scalar_t2 *>(regs_b);
      scalar_t2 bs_f16_ptr[kNumGroupsB][2];
      PRAGMA_UNROLL
      for (uint32_t g = 0; g < kNumGroupsB; g++) {
        scalar_t *bs_half_ptr = reinterpret_cast<scalar_t *>(ElementBS::kBits == 8 ? &dq_bs[g][j] : &bs[buffer_id][g][j]);
        PRAGMA_UNROLL
        for (uint32_t i = 0; i < 2; i++) {
          bs_f16_ptr[g][i] = this->num2num2(bs_half_ptr[i]);
        }
      };

      // Each warp apply bs or/and zp on a 16x16 weight block per time
      // The thread_id 0 process the following weight index of each block
      // [(0, 1), (0, 2)], [(0, 8), (0, 9)]
      // [(8, 1), (8, 2)], [(8, 8), (8, 9)]
      PRAGMA_UNROLL
      for (uint32_t i = 0; i < 2; i++) {
        PRAGMA_UNROLL
        for (uint32_t k = 0; k < 2; k++) {
          uint32_t g = k % kNumGroupsB;
          scalar_t2 bs_single = bs_f16_ptr[g][i];
          b_f16_ptr[i * 2 + k] = __hmul2(b_f16_ptr[i * 2 + k], bs_single);
        };
      };
    };
  };

  CUDA_INLINE
  void may_process_as_and_bs_before_apply_on_c(uint32_t m, uint32_t n, uint32_t k, uint32_t iter_id) {

    uint32_t k_index = (iter_id * kPartMmaShapeK) + k * MmaShape::K;
    uint32_t buffer_id = iter_id % 2;
    bool is_new_as_group = kIsGroupInputScale && k_index % kInputScaleGroupSize == 0;
    if (m == 0 && n == 0 && is_new_as_group) {
      if constexpr (kIsF16Accum) {
        float2 *as_float2_ptr = reinterpret_cast<float2 *>(as[buffer_id]);
        float *as_float_ptr = reinterpret_cast<float *>(as[buffer_id]);
        scalar_t2 *as_scalar2_ptr = reinterpret_cast<scalar_t2 *>(q_as);
        PRAGMA_UNROLL
        for (uint32_t i = 0; i < WarpShape::M / 8; i++) {
          if constexpr (kNumGroupsA == 1 && kUseWgmma) {
            as_scalar2_ptr[i] = this->float22num2(as_float2_ptr[i]);
          } else if constexpr (kNumGroupsA == 1) {
            as_scalar2_ptr[i] = this->float2num2(as_float_ptr[i]);
          } else if constexpr (kNumGroupsA == 2 && kUseWgmma) {
            as_scalar2_ptr[i] = this->floats2num2(as_float_ptr[i * 4 + k], as_float_ptr[i * 4 + 2 + k]);
          } else if constexpr (kNumGroupsA == 2) {
            as_scalar2_ptr[i] = this->float2num2(as_float_ptr[i * 2 + k]);
          }

          if constexpr (kExpOffset.y && !kIsGroupWeightScale) {
            scalar_t2 scale_factor = prepare_exp_scale_factor<scalar_t2, kExpOffset.y>();
            as_scalar2_ptr[i] = __hmul2(as_scalar2_ptr[i], scale_factor);
          }
        };
      };
    }

    bool is_new_bs_group = kIsGroupWeightScale && k_index % kWeightScaleGroupSize == 0;
    if (m == 0 && n == 0 && is_new_bs_group) {
      if constexpr (kIsF16Accum && ElementBS::kBits == 16) {
        scalar_t2 *dq_bs_scalar2_ptr = reinterpret_cast<scalar_t2 *>(dq_bs);
        scalar_t2 *bs_scalar2_ptr = reinterpret_cast<scalar_t2 *>(bs[buffer_id][k]);

        if constexpr (kExpOffset.y) {
          scalar_t2 scale_factor = prepare_exp_scale_factor<scalar_t2, kExpOffset.y>();
          PRAGMA_UNROLL
          for (uint32_t i = 0; i < kNumBSPerGroup / 2; i++)
            dq_bs_scalar2_ptr[i] = __hmul2(bs_scalar2_ptr[i], scale_factor);
        }
      } else if constexpr (kIsF16Accum && ElementBS::kBits == 8 && kIsGroupWeightScale) {
        PRAGMA_UNROLL
        for (uint32_t i = 0; i < CEIL_DIV(kNumBSPerGroup, 8); i++) {
          dequant<ElementBS, ElementC>(bs[buffer_id][k], dq_bs[0], i);

          scalar_t *dq_bs_scalar_ptr = reinterpret_cast<scalar_t *>(dq_bs);
          PRAGMA_UNROLL
          for (uint32_t j = 0; j < 2; j++) {
            scalar_t tmp = dq_bs_scalar_ptr[2 + 4 * j];
            dq_bs_scalar_ptr[2 + 4 * j] = dq_bs_scalar_ptr[1 + 4 * j];
            dq_bs_scalar_ptr[1 + 4 * j] = tmp;
          }
        }

        scalar_t2 *dq_bs_scalar2_ptr = reinterpret_cast<scalar_t2 *>(dq_bs);
        constexpr uint32_t exp_offset = get_dtype_dequant_exp_offset<ElementC, ElementBS>();
        scalar_t2 scale_factor = prepare_exp_scale_factor<scalar_t2, exp_offset>();
        PRAGMA_UNROLL
        for (uint32_t i = 0; i < kNumBSPerGroup / 2; i++)
          dq_bs_scalar2_ptr[i] = __hmul2(dq_bs_scalar2_ptr[i], scale_factor);

        if constexpr (kExpOffset.y) {
          scalar_t2 scale_factor = prepare_exp_scale_factor<scalar_t2, kExpOffset.y>();
          PRAGMA_UNROLL
          for (uint32_t i = 0; i < kNumBSPerGroup / 2; i++)
            dq_bs_scalar2_ptr[i] = __hmul2(dq_bs_scalar2_ptr[i], scale_factor);
        }

      } else if constexpr (!kIsF16Accum && ElementBS::kBits == 8) {
        using F8x4 = typename F8Conversion<ElementBS>::scalar_t4;
        F8x4 *bs_vals = reinterpret_cast<F8x4 *>(bs[buffer_id][k]);
        float4 *dq_bs_vals = reinterpret_cast<float4 *>(dq_bs);
        PRAGMA_UNROLL
        for (uint32_t i = 0; i < kNumBSPerGroup / 4; i++)
          dq_bs_vals[i] = F8Conversion<ElementBS>::num42float4(bs_vals[i]);
      } else if constexpr (!kIsF16Accum && ElementBS::kBits == 16) {
        using F16x2 = typename F16Conversion<ElementBS>::scalar_t2;
        F16x2 *bs_vals = reinterpret_cast<F16x2 *>(bs[buffer_id][k]);
        float2 *dq_bs_vals = reinterpret_cast<float2 *>(dq_bs);
        PRAGMA_UNROLL
        for (uint32_t i = 0; i < kNumBSPerGroup / 2; i++)
          dq_bs_vals[i] = F16Conversion<ElementBS>::num22float2(bs_vals[i]);
      };
    }
  };

  CUDA_INLINE
  void may_apply_as_and_bs_on_mma_c(uint32_t *regs_c_ptr, uint32_t m, uint32_t n, uint32_t k, uint32_t iter_id) {
    if constexpr (ElementA::kBits == 16 || !kIsGroupInputScale && !kIsGroupWeightScale) return;
    if constexpr (kUseWgmma) return;

    may_process_as_and_bs_before_apply_on_c(m, n, k, iter_id);

    constexpr uint32_t kWarpItersK = WarpShape::K / kPartMmaShapeK;
    uint32_t buffer_id = iter_id % 2;
    uint32_t k_index = iter_id * kPartMmaShapeK + (k + 1) * MmaShape::K;
    uint32_t is_last_iter = iter_id == (kWarpItersK - 1);
    uint32_t is_as_group_end = kIsGroupInputScale && k_index % kInputScaleGroupSize == 0;
    uint32_t is_bs_group_end = kIsGroupWeightScale && k_index % kWeightScaleGroupSize == 0;

    bool should_apply_as = kIsGroupInputScale && (is_last_iter || is_as_group_end);
    bool should_apply_bs = kIsGroupWeightScale && (is_last_iter || is_bs_group_end);
    if (!should_apply_as && !should_apply_bs) return;

    using CRegistersArrayType = typename MmaOpClass::CRegisters[2][WarpShape::M / MmaShape::M][WarpShape::N / MmaShape::N];
    auto &regs_c = *reinterpret_cast<CRegistersArrayType *>(regs_c_ptr);

    PRAGMA_UNROLL
    for (uint32_t index = 0; index < MmaShape::M * MmaShape::N / 64; index++) {
      uint32_t inner_m = index % (MmaShape::M / 8);
      uint32_t inner_n = index / (MmaShape::M / 8);

      if constexpr (kIsF16Accum) {
        scalar_t2 &part_regs_c0 = reinterpret_cast<scalar_t2 *>(regs_c[0][m][n])[index];
        scalar_t2 &part_regs_c1 = reinterpret_cast<scalar_t2 *>(regs_c[1][m][n])[index];

        scalar_t2 as_val = reinterpret_cast<scalar_t2 *>(q_as)[m * MmaShape::M / 8 + inner_m];
        scalar_t2 bs_val;
        if constexpr (ElementBS::kBits == 8 || kExpOffset.y) {
          bs_val = reinterpret_cast<scalar_t2 *>(dq_bs)[n * MmaShape::N / 8 + inner_n];
        } else {
          bs_val = reinterpret_cast<scalar_t2 *>(bs[buffer_id][k])[n * MmaShape::N / 8 + inner_n];
        }

        if constexpr (kIsGroupInputScale && kIsGroupWeightScale) {
          part_regs_c1 = __hfma2(__hmul2(part_regs_c0, as_val), bs_val, part_regs_c1);
        } else if constexpr (kIsGroupInputScale) {
          part_regs_c1 = __hfma2(part_regs_c0, as_val, part_regs_c1);
        } else if constexpr (kIsGroupWeightScale) {
          part_regs_c1 = __hfma2(part_regs_c0, bs_val, part_regs_c1);
        }
        reinterpret_cast<uint32_t *>(&part_regs_c0)[0] = 0;

      } else {
        int2 &part_int_regs_c0 = reinterpret_cast<int2 *>(regs_c[0][m][n])[index];
        float2 &part_regs_c0 = reinterpret_cast<float2 *>(regs_c[0][m][n])[index];
        float2 &part_regs_c1 = reinterpret_cast<float2 *>(regs_c[1][m][n])[index];

        if constexpr (std::is_same<ValTypeC, int32_t>::value) {
          part_regs_c0.x = __int2float_rn(part_int_regs_c0.x);
          part_regs_c0.y = __int2float_rn(part_int_regs_c0.y);
        }

        uint32_t group_id = kPartMmaShapeK > kInputScaleGroupSize ? k : 0;
        float &as_val = reinterpret_cast<float *>(as[buffer_id][m * MmaShape::M / 8 + inner_m])[group_id];
        float2 &bs_vals = reinterpret_cast<float2 *>(dq_bs)[n * MmaShape::N / 8 + inner_n];

        if constexpr (kIsGroupInputScale && kIsGroupWeightScale) {
          part_regs_c1.x += as_val * bs_vals.x * part_regs_c0.x;
          part_regs_c1.y += as_val * bs_vals.y * part_regs_c0.y;
        } else if constexpr (kIsGroupInputScale) {
          part_regs_c1.x += as_val * part_regs_c0.x;
          part_regs_c1.y += as_val * part_regs_c0.y;
        } else if constexpr (kIsGroupWeightScale) {
          part_regs_c1.x += bs_vals.x * part_regs_c0.x;
          part_regs_c1.y += bs_vals.y * part_regs_c0.y;
        }

        part_regs_c0.x = 0;
        part_regs_c0.y = 0;
      }
    }
  }

  CUDA_INLINE
  void may_apply_as_and_bs_on_wgmma_c(uint32_t *regs_c_ptr, uint32_t m, uint32_t k, uint32_t iter_id) {
    if constexpr (ElementA::kBits == 16 || !kIsGroupInputScale && !kIsGroupWeightScale) return;
    if constexpr (!kUseWgmma) return;

    may_process_as_and_bs_before_apply_on_c(m, 0, k, iter_id);

    constexpr uint32_t kWarpItersK = WarpShape::K / kPartMmaShapeK;
    uint32_t buffer_id = iter_id % 2;
    uint32_t k_index = iter_id * kPartMmaShapeK + (k + 1) * MmaShape::K;
    uint32_t is_last_iter = iter_id == (kWarpItersK - 1);
    uint32_t is_as_group_end = kIsGroupInputScale && k_index % kInputScaleGroupSize == 0;
    uint32_t is_bs_group_end = kIsGroupWeightScale && k_index % kWeightScaleGroupSize == 0;

    bool should_apply_as = kIsGroupInputScale && (is_last_iter || is_as_group_end);
    bool should_apply_bs = kIsGroupWeightScale && (is_last_iter || is_bs_group_end);
    if (!should_apply_as && !should_apply_bs) return;

    using CRegistersArrayType = typename MmaOpClass::CRegisters[2][WarpShape::N * 4 / MmaShape::M][WarpShape::M / MmaShape::N];
    auto &regs_c = *reinterpret_cast<CRegistersArrayType *>(regs_c_ptr);

    PRAGMA_UNROLL
    for (uint32_t index = 0; index < MmaShape::N * 16 / 64; index++) {
      uint32_t inner_n = index / 2;
      uint32_t inner_m = index % 2;

      if constexpr (kIsF16Accum) {
        scalar_t2 &part_regs_c0 = reinterpret_cast<scalar_t2 *>(regs_c[0][m][0])[index];
        scalar_t2 &part_regs_c1 = reinterpret_cast<scalar_t2 *>(regs_c[1][m][0])[index];

        scalar_t2 as_val = reinterpret_cast<scalar_t2 *>(q_as)[inner_n];
        scalar_t2 bs_val;
        if constexpr (ElementBS::kBits == 8 || kExpOffset.y) {
          bs_val = this->num2num2(reinterpret_cast<scalar_t *>(dq_bs)[m * 2 + inner_m]);
        } else {
          bs_val = this->num2num2(reinterpret_cast<scalar_t *>(bs[buffer_id])[m * 2 + inner_m]);
        }

        if constexpr (kIsGroupInputScale && kIsGroupWeightScale) {
          part_regs_c1 = __hfma2(__hmul2(part_regs_c0, as_val), bs_val, part_regs_c1);
        } else if constexpr (kIsGroupInputScale) {
          part_regs_c1 = __hfma2(part_regs_c0, as_val, part_regs_c1);
        } else if constexpr (kIsGroupWeightScale) {
          part_regs_c1 = __hfma2(part_regs_c0, bs_val, part_regs_c1);
        }

        reinterpret_cast<uint32_t *>(&part_regs_c0)[0] = 0;

      } else {
        int2 &part_int_regs_c0 = reinterpret_cast<int2 *>(regs_c[0][m][0])[index];
        float2 &part_regs_c0 = reinterpret_cast<float2 *>(regs_c[0][m][0])[index];
        float2 &part_regs_c1 = reinterpret_cast<float2 *>(regs_c[1][m][0])[index];

        if constexpr (std::is_same<ValTypeC, int32_t>::value) {
          part_regs_c0.x = __int2float_rn(part_int_regs_c0.x);
          part_regs_c0.y = __int2float_rn(part_int_regs_c0.y);
        }

        uint32_t group_id = kPartMmaShapeK > kInputScaleGroupSize ? k : 0;
        float2 &as_vals = reinterpret_cast<float2 *>(as[buffer_id][inner_n * 2])[group_id];
        float &bs_val = reinterpret_cast<float *>(dq_bs)[m * 2 + inner_m];

        if constexpr (kIsGroupInputScale && kIsGroupWeightScale) {
          part_regs_c1.x += as_vals.x * bs_val * part_regs_c0.x;
          part_regs_c1.y += as_vals.y * bs_val * part_regs_c0.y;
        } else if constexpr (kIsGroupInputScale) {
          part_regs_c1.x += as_vals.x * part_regs_c0.x;
          part_regs_c1.y += as_vals.y * part_regs_c0.y;
        } else if constexpr (kIsGroupWeightScale) {
          part_regs_c1.x += bs_val * part_regs_c0.x;
          part_regs_c1.y += bs_val * part_regs_c0.y;
        }

        part_regs_c0.x = 0;
        part_regs_c0.y = 0;
      }
    }
  }

  template <class T = uint32_t>
  CUDA_INLINE T *regs_bs_as_ptr(uint32_t buffer_id) {
    return reinterpret_cast<T *>(bs[buffer_id]);
  };

  template <class T = uint32_t>
  CUDA_INLINE T *regs_as_as_ptr(uint32_t buffer_id) {
    return reinterpret_cast<T *>(as[buffer_id]);
  };

  template <class T = uint32_t>
  CUDA_INLINE T *regs_zp_as_ptr(uint32_t buffer_id) {
    return reinterpret_cast<T *>(zp[buffer_id]);
  };
};
