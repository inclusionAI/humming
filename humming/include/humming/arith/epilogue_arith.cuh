#pragma once

#include <humming/arith/activation.cuh>
#include <humming/arith/exp_offset.cuh>
#include <humming/datatype/base_conversion.cuh>
#include <humming/datatype/dequant.cuh>
#include <humming/datatype/dtypes.cuh>
#include <humming/utils/all.cuh>

template <
    class MmaOpClass,
    class BlockShape, class WarpShape,
    class ElementA, class ElementB, class ElementC, class ElementBS,
    class SchedulerConfig, class EpilogueConfig, class QuantParamConfig, class MoEConfig>
class EpilogueArithmetic : F16Conversion<ElementC> {
private:
  using scalar_t = typename F16Conversion<ElementC>::scalar_t;
  using scalar_t2 = typename F16Conversion<ElementC>::scalar_t2;

  static constexpr bool kUseStreamK = SchedulerConfig::kUseStreamK;
  static constexpr bool kIsF16Accum = MmaOpClass::kCTypeBits == 16;
  static constexpr bool kHasBias = EpilogueConfig::kHasBias;
  static constexpr bool kHasInputScale = QuantParamConfig::kHasInputScale;
  static constexpr bool kHasWeightScale = QuantParamConfig::kHasWeightScale;
  static constexpr bool kIsGroupInputScale = kHasInputScale && QuantParamConfig::kInputScaleGroupSize > 0;
  static constexpr bool kIsGroupWeightScale = kHasWeightScale && QuantParamConfig::kWeightScaleGroupSize > 0;
  static constexpr bool kIsChannelInputScale = kHasInputScale && QuantParamConfig::kInputScaleGroupSize == 0;
  static constexpr bool kIsChannelWeightScale = kHasWeightScale && QuantParamConfig::kWeightScaleGroupSize == 0;
  static constexpr bool kHasGlobalScale = QuantParamConfig::kHasGlobalScale;
  static constexpr bool kHasDynamicZeroPoint = QuantParamConfig::kHasDynamicZeroPoint;
  static constexpr bool kIsMoEDown = MoEConfig::kIsMoEDown;

  static constexpr bool kHasActivation = EpilogueConfig::kActivationType != ActivationType::NONE;
  static constexpr bool kHasGLUActivation = EpilogueConfig::kActivationType == ActivationType::SILU_GLU ||
                                            EpilogueConfig::kActivationType == ActivationType::CUSTOM_GLU;
  static constexpr bool kHasIndentityActivation = kHasActivation && !kHasGLUActivation;

  static constexpr uint2 kExpOffset = get_epilogue_exp_offset<
      ElementA, ElementB, ElementC, ElementBS, kHasDynamicZeroPoint,
      kIsF16Accum, kIsGroupInputScale, kIsGroupWeightScale>();

  static constexpr uint32_t kSizeAS = WarpShape::M / 8;
  static constexpr uint32_t kSizeBS = WarpShape::N / 4 * ElementBS::kBits / 32;
  static constexpr uint32_t kSizeDequantBS = kSizeBS * 16 / ElementBS::kBits;
  static constexpr uint32_t kSizeBias = WarpShape::N / 4 * 16 / 32;

public:
  uint32_t as[kSizeAS];
  uint32_t bs[MAX(kSizeBS, 2)];
  uint32_t dq_bs[MAX(kSizeDequantBS, 4)];
  uint32_t bias[kSizeBias];
  uint32_t topk_weights[kSizeAS];
  uint32_t gs = 0;
  uint32_t _dummy;

  CUDA_INLINE
  void may_apply_activation_on_smem_write(uint32_t &regs, uint32_t slice_count) {
    float2 regs_float2 = this->num22float2(*reinterpret_cast<scalar_t2 *>(&regs));
    float *regs_float_ptr = reinterpret_cast<float *>(&regs_float2);

    if (!kUseStreamK || slice_count == 1) {
      if constexpr (kHasIndentityActivation) {
        regs_float_ptr[0] = activation_func<EpilogueConfig::kActivationType>(regs_float_ptr[0]);
        regs_float_ptr[1] = activation_func<EpilogueConfig::kActivationType>(regs_float_ptr[1]);
        reinterpret_cast<scalar_t2 *>(&regs)[0] = this->float22num2(regs_float2);
      } else if constexpr (kHasGLUActivation) {
        regs_float_ptr[0] = activation_func<EpilogueConfig::kActivationType>(regs_float2);
        reinterpret_cast<scalar_t2 *>(&regs)[0] = this->float22num2(regs_float2);
      }
    }
  };

  CUDA_INLINE
  void apply_activation_on_gmem_write(int4 &regs) {
    float2 regs_float2_ptr[4];
    float *regs_float_ptr = reinterpret_cast<float *>(regs_float2_ptr);
    scalar_t2 *regs_f162_ptr = reinterpret_cast<scalar_t2 *>(&regs);

    PRAGMA_UNROLL
    for (uint32_t i = 0; i < 4; i++) {
      regs_float2_ptr[i] = this->num22float2(regs_f162_ptr[i]);
    }

    if constexpr (kHasIndentityActivation) {
      PRAGMA_UNROLL
      for (uint32_t i = 0; i < 8; i++) {
        regs_float_ptr[i] = activation_func<EpilogueConfig::kActivationType>(regs_float_ptr[i]);
      }

      PRAGMA_UNROLL
      for (uint32_t i = 0; i < 4; i++) {
        regs_f162_ptr[i] = this->float22num2(regs_float2_ptr[i]);
      }

    } else if constexpr (kHasGLUActivation) {
      PRAGMA_UNROLL
      for (uint32_t i = 0; i < 4; i++) {
        regs_float_ptr[i] = activation_func<EpilogueConfig::kActivationType>(regs_float2_ptr[i]);
      }

      PRAGMA_UNROLL
      for (uint32_t i = 0; i < 2; i++) {
        regs_f162_ptr[i] = this->float22num2(regs_float2_ptr[i]);
      }
    }
  };

  CUDA_INLINE
  void may_process_on_smem_write(uint32_t row, uint32_t col) {
    if constexpr (kIsChannelInputScale && kIsF16Accum) {
      if (row == 0 && col == 0) {
        PRAGMA_UNROLL
        for (uint32_t i = 0; i < kSizeAS; i++) {
          reinterpret_cast<scalar_t2 *>(as)[i] = this->float2num2(reinterpret_cast<float *>(as)[i]);
        };
      };
    };

    if constexpr (kIsMoEDown) {
      if (row == 0 && col == 0) {
        PRAGMA_UNROLL
        for (uint32_t i = 0; i < kSizeAS; i++) {
          reinterpret_cast<scalar_t2 *>(topk_weights)[i] = this->float2num2(reinterpret_cast<float *>(topk_weights)[i]);
        };
      };
    };

    if constexpr (kIsChannelWeightScale && ElementBS::kBits == 8) {
      if (row == 0 && col == 0) {
        PRAGMA_UNROLL
        for (uint32_t i = 0; i < CEIL_DIV(WarpShape::N, 32); i++) {
          dequant<ElementBS, ElementC>(bs, dq_bs + i * 4, i);
          scalar_t *dq_bs_scalar_ptr = reinterpret_cast<scalar_t *>(dq_bs + i * 4);

          PRAGMA_UNROLL
          for (uint32_t j = 0; j < 2; j++) {
            scalar_t tmp = dq_bs_scalar_ptr[2 + 4 * j];
            dq_bs_scalar_ptr[2 + 4 * j] = dq_bs_scalar_ptr[1 + 4 * j];
            dq_bs_scalar_ptr[1 + 4 * j] = tmp;
          }
        }

        const scalar_t2 scale_factor = prepare_exp_scale_factor<scalar_t2, kExpOffset.y>();
        scalar_t2 *dq_bs_scalar2_ptr = reinterpret_cast<scalar_t2 *>(dq_bs);

        PRAGMA_UNROLL
        for (uint32_t j = 0; j < CEIL_DIV(WarpShape::N, 8); j++) {
          dq_bs_scalar2_ptr[j] = __hmul2(dq_bs_scalar2_ptr[j], scale_factor);
        }
      };
    };

    if constexpr (kHasGlobalScale) {
      if (row == 0 && col == 0) {
        scalar_t2 &gs_f16 = *reinterpret_cast<scalar_t2 *>(&gs);
        float &gs_f32 = *reinterpret_cast<float *>(&gs);
        gs_f16 = this->float2num2(gs_f32);
        if constexpr (kExpOffset.x && !(kIsChannelWeightScale && ElementBS::kBits == 8)) {
          const scalar_t2 scale_factor = prepare_exp_scale_factor<scalar_t2, kExpOffset.x>();
          gs_f16 = __hmul2(gs_f16, scale_factor);
        }
      }
    }
  };

  CUDA_INLINE
  void may_apply_f32_as_on_smem_write(float2 &regs, uint32_t row) {
    float *as_f32_ptr = reinterpret_cast<float *>(as);
    if constexpr (kIsChannelInputScale && !kIsF16Accum) {
      regs.x = regs.x * as_f32_ptr[row];
      regs.y = regs.y * as_f32_ptr[row];
    };
  };

  CUDA_INLINE
  void may_apply_on_smem_write(uint32_t &regs, uint32_t row, uint32_t col) {
    may_process_on_smem_write(row, col);

    auto apply_gs_or_exp_offset = [&]() {
      if constexpr (kHasGlobalScale) {
        scalar_t2 &gs_f16 = *reinterpret_cast<scalar_t2 *>(&gs);
        scalar_t2 *b_f16_ptr = reinterpret_cast<scalar_t2 *>(&regs);
        b_f16_ptr[0] = __hmul2(b_f16_ptr[0], gs_f16);
      } else if constexpr (kExpOffset.x && !kHasGlobalScale) {
        const scalar_t2 scale_factor = prepare_exp_scale_factor<scalar_t2, kExpOffset.x>();
        scalar_t2 *b_f16_ptr = reinterpret_cast<scalar_t2 *>(&regs);
        b_f16_ptr[0] = __hmul2(b_f16_ptr[0], scale_factor);
      }
    };

    if constexpr (!kIsF16Accum) apply_gs_or_exp_offset();

    scalar_t2 *as_half2 = reinterpret_cast<scalar_t2 *>(as);
    scalar_t2 *bs_half2 = reinterpret_cast<scalar_t2 *>(ElementBS::kBits == 8 ? dq_bs : bs);
    scalar_t2 *bias_half2 = reinterpret_cast<scalar_t2 *>(bias);
    scalar_t2 *regs_half2 = reinterpret_cast<scalar_t2 *>(&regs);

    if constexpr (kIsChannelInputScale && kIsF16Accum) {
      regs_half2[0] = __hmul2(regs_half2[0], as_half2[row]);
    };

    if constexpr (kIsChannelWeightScale && kHasBias && !kIsF16Accum) {
      regs_half2[0] = __hfma2(regs_half2[0], bs_half2[col], bias_half2[col]);
    } else if constexpr (kIsChannelWeightScale) {
      regs_half2[0] = __hmul2(regs_half2[0], bs_half2[col]);
    } else if constexpr (kHasBias) {
      regs_half2[0] = __hadd2(regs_half2[0], bias_half2[col]);
    };

    if constexpr (kIsF16Accum) apply_gs_or_exp_offset();

    if constexpr (kIsF16Accum && kHasBias) {
      regs_half2[0] = __hadd2(regs_half2[0], bias_half2[col]);
    }

    if constexpr (kIsMoEDown) {
      scalar_t2 score = *reinterpret_cast<scalar_t2 *>(&topk_weights[row]);
      regs_half2[0] = __hmul2(regs_half2[0], score);
    }
  };

  template <class T = uint32_t>
  CUDA_INLINE T *regs_bs_as_ptr() {
    return reinterpret_cast<T *>(bs);
  };

  template <class T = uint32_t>
  CUDA_INLINE T *regs_as_as_ptr() {
    return reinterpret_cast<T *>(as);
  };

  template <class T = uint32_t>
  CUDA_INLINE T *regs_bias_as_ptr() {
    return reinterpret_cast<T *>(bias);
  };

  template <class T = uint32_t>
  CUDA_INLINE T *regs_topk_weights_as_ptr() {
    return reinterpret_cast<T *>(topk_weights);
  };
};
